Taches pour ce samedi
    Fred:
        * continue Segmentation
        * optimization size dictionaire

    Michele:
        * regarder comment comment on peut enlever de features a SIFT (reduire le 128)
            1 image == M x 128 (m descriptors, chaque desc a 128 floats)
                - mesure dependance entre quantitatif continue et label?
                    - https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest
                    - https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html

                - https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html
                    DecisionTreeClassifier()
                    RandomForest()

                - https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html

    JFG:
        * regarder pourquoi OpenCV SIFT marche pas pour Frederic
        * revoir les ensembles, les equilibrer et donner cette info dans le pipeline
          (on garde grape pour le moment)
        * assurer que le choix du data est propager dans le pipeline (config_overrides.json)
        * finir le notebook training

Objectifs:
    * Extraction features des images
        - squelette est presentation

    * Construire le dictionnaire
        - recherche sur perf debutee
        - interface avec les autres modules passe par config_overrides.json (tout est manuel)
        - les bovw sont serialise en avec pickle
        - il faudrait starter analyse pour resampler et analyser les correlations entre features
        - investiguer visualisation pour aider a comprendre les correlations (t-sne)?
        - investiguer d'autres choix de features?

    * Contruire pour chaque image un histogramme en fonction du dictionnaire
        - traitement par image est fait en partie, il faudrait ajouter le traitement en lot
        - saver dans fichier hd5 resultats intermediaires ou juste faire les calculs a la demande?

    * Entrainer KNN et Random Forest

    * Sauvegarder des modeles

    * Wrapper dans "application" (widgets notebook?)

    * Optimization des hyperparametres (les valeurs qui sont dans config_overrides.json)
        * choix du type de features (ORB ou SIFT)
        * determiner nombre de features optimales
        * Ajouter un label 'Ca marche pas -  inclassifiable'

    * Evaluer

    * Rapport + presentation

Stretch goal:
    * Segmentation background/foreground pour feature extraction
    * Size optimal pour dictionnaire (trouver algo et resampler features)
    * TF-IDF pour l'histogramme
    * Passer SIFT Features en half float si besoin
