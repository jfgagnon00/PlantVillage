{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d914ec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T22:55:25.029792Z",
     "start_time": "2023-04-26T22:55:21.475229Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#\n",
    "# import utilitaires\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import helpers as hlp\n",
    "import helpers.features as feat\n",
    "import helpers.dataset.PlantVillage as pv\n",
    "import helpers.split as split\n",
    "import helpers.visual_words as vw\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# enlever FutureWarning de sklearn \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "357224e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T22:55:25.687370Z",
     "start_time": "2023-04-26T22:55:25.031920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 3711\n",
      "Test size 928\n",
      "\n",
      "Dataset (60343, 4)\n",
      "\n",
      "ORB features (30460434, 32)\n",
      "ORB train features (2149542, 32)\n",
      "ORB test features (538838, 32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# parametres\n",
    "#\n",
    "configs = hlp.get_configs(\"config_overrides.json\")\n",
    "\n",
    "\n",
    "#\n",
    "# train/test sets\n",
    "# \n",
    "train, test = split.load(configs.split)\n",
    "print(\"Train size\", len(train))\n",
    "print(\"Test size\", len(test))\n",
    "print()\n",
    "\n",
    "\n",
    "#\n",
    "# dataset PlantVillage\n",
    "#\n",
    "pv_dataset = pv.load(configs.plant_village)\n",
    "pv_dataframe = pv_dataset.dataframe\n",
    "\n",
    "print(\"Dataset\", pv_dataframe.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "#\n",
    "# orb\n",
    "#\n",
    "orb_features = feat.load(configs.orb, None)\n",
    "\n",
    "print(\"ORB features\", orb_features.features.shape)\n",
    "print(\"ORB train features\", orb_features.train_features.shape)\n",
    "print(\"ORB test features\", orb_features.test_features.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30849d3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T22:55:37.946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construction Bag of Visual Words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# visual words orb\n",
    "#\n",
    "\n",
    "configs.orb_bovw.force_generate = True\n",
    "\n",
    "if \"orb_bovw\" in locals():\n",
    "    del orb_bovw\n",
    "\n",
    "with hlp.Profile() as orb_bovw_profile:\n",
    "    orb_bovw = vw.load_bovw(configs.orb_bovw, orb_features.train_features[::75, ...])\n",
    "\n",
    "print(\"ORB BoVW loading:\", orb_bovw_profile.round_duration(), \"s\")\n",
    "# print(\"ORB BoVW clusters:\", orb_bovw.model.cluster_centers_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0362b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs.orb_dataset_vw.force_generate = True\n",
    "\n",
    "if \"orb_pv_vw\" in locals():\n",
    "    del orb_pv_vw\n",
    "\n",
    "with hlp.Profile() as orb_pv_vw_loading:\n",
    "    orb_pv_vw = vw.load_dataset_vw(configs.orb_dataset_vw,\n",
    "                                   orb_features,\n",
    "                                   configs.orb_bovw,\n",
    "                                   orb_bovw,\n",
    "                                   train,\n",
    "                                   test)\n",
    "    \n",
    "print(f\"ORB PlantVillage Visual Word loading:\", orb_pv_vw_loading.round_duration(), \"s\")\n",
    "print(\"ORB PlantVillage Visual Words\", orb_pv_vw.vw_freqs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafcf7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# visualiser quelques visual words orb\n",
    "#\n",
    "\n",
    "SAMPLES_GRID = (4, 2)\n",
    "SAMPLES_COUNT = SAMPLES_GRID[0]\n",
    "FIGSIZE = (10, SAMPLES_GRID[0] * 2.5)\n",
    "\n",
    "def affiche_image(image_indices, visual_words, idf, features, pv_vw):\n",
    "    x_range = range(visual_words.n_clusters)\n",
    "    figure, axes = plt.subplots(SAMPLES_GRID[0],\n",
    "                                SAMPLES_GRID[1],\n",
    "                                figsize=FIGSIZE,\n",
    "                                width_ratios=[1, 3])\n",
    "\n",
    "    for i, image_index in enumerate(image_indices):\n",
    "        key_points = features.index_to_key_points[str(image_index)][...]\n",
    "        visual_words_freq = pv_vw.index_to_vw_freqs[str(image_index)][...].flatten()\n",
    "        tf_idf = np.multiply(idf, visual_words_freq)\n",
    "\n",
    "        image = pv_dataset.get_image(image_index)\n",
    "        image_key_points = visual_words.draw_key_points(image, key_points)\n",
    "\n",
    "        species, disease = pv_dataframe.loc[image_index, [\"species\", \"disease\"]]\n",
    "        disease = pv.attribute_prettify(species, disease)\n",
    "        title = \"\\n\".join([species, disease])\n",
    "\n",
    "        plt.sca(axes[i, 0])\n",
    "        plt.gca().set_ylabel(title)\n",
    "        plt.gca().set_xticks([])\n",
    "        plt.gca().set_yticks([])\n",
    "        plt.imshow(image_key_points)\n",
    "\n",
    "        plt.sca(axes[i, 1])\n",
    "        plt.bar(x_range, tf_idf)\n",
    "\n",
    "    axes[0, 0].set_title(\"Key Points\")\n",
    "    axes[0, 1].set_title(\"tf-idf Visual Words\")\n",
    "    \n",
    "\n",
    "# selectionner quelques images aleatoirement\n",
    "random.seed(33)\n",
    "image_indices = random.choices(train, k=SAMPLES_COUNT)\n",
    "image_indices.sort()\n",
    "\n",
    "affiche_image(image_indices,\n",
    "              vw.VisualWords(configs.orb, orb_bovw.model),\n",
    "              np.ravel(orb_bovw.idf),\n",
    "              orb_features, \n",
    "              orb_pv_vw)\n",
    "\n",
    "plt.suptitle(\"ORB\")\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213570f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_knn(model_filename, train_x, train_y):\n",
    "    \"\"\"\n",
    "    Utilitaire pour entrainer un model et le serialiser\n",
    "    \"\"\"\n",
    "    if False and os.path.exists(model_filename):\n",
    "        # charger le modele pre-entrainer\n",
    "        with open(model_filename, \"rb\") as file:\n",
    "            pipe = pickle.load(file)\n",
    "    else:\n",
    "        # trouver meilleurs parametres pour n_neighbors\n",
    "        CROSS_VALIDATION_FOLDS = 5\n",
    "        \n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                         (\"knn\", KNeighborsClassifier())])\n",
    "\n",
    "        pipe_parameters = {\"knn__n_neighbors\": range(1, 5)}\n",
    "\n",
    "        grid_search = GridSearchCV(pipe, \n",
    "                                  pipe_parameters, \n",
    "                                  scoring=\"accuracy\",\n",
    "                                  refit=True,\n",
    "                                  cv=CROSS_VALIDATION_FOLDS)\n",
    "\n",
    "        with hlp.Profile() as grid_search_time:\n",
    "            grid_search.fit(train_x, train_y)\n",
    "\n",
    "        # sauvegarde du meilleur modele knn\n",
    "        pipe = grid_search.best_estimator_\n",
    "        head, _ = os.path.split(model_filename)\n",
    "        os.makedirs(head, exist_ok=True)\n",
    "        with open(model_filename, \"wb\") as file:\n",
    "            pickle.dump(pipe, file)\n",
    "\n",
    "        # afficher quelques stats\n",
    "        print(\"Search time:\", grid_search_time.round_duration(), \"s\")\n",
    "        print(\"Best params\", grid_search.best_params_)\n",
    "        print(\"Train score:\", grid_search.best_score_.round(4))\n",
    "        print()\n",
    "            \n",
    "    return pipe\n",
    "\n",
    "def analysis_knn(method_name, pipe, test_x, test_y):\n",
    "    test_y_hat = pipe.predict(test_x)\n",
    "\n",
    "    print(method_name, \"KNN n_neighbors:\", pipe.named_steps[\"knn\"].n_neighbors)\n",
    "    print(method_name, \"KNN test score:\", accuracy_score(test_y, test_y_hat).round(4))\n",
    "    print(\"Effectifs test set:\")\n",
    "    print( test_y.value_counts() )\n",
    "\n",
    "    # confusion matrix\n",
    "    plt.figure(figsize=(5, 4))\n",
    "\n",
    "    cm = confusion_matrix(test_y, test_y_hat)\n",
    "    cmd = ConfusionMatrixDisplay(cm, display_labels=pipe.classes_)\n",
    "    cmd.plot(ax=plt.gca())\n",
    "    cmd.im_.colorbar.remove()\n",
    "\n",
    "    plt.title(f\"{method_name} - KNN - Confusion matrix\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#\n",
    "# mettre le data dans une forme acceptable pour sklearn\n",
    "#\n",
    "train_y = pv_dataframe.loc[train, \"label\"]\n",
    "test_y = pv_dataframe.loc[test, \"label\"]\n",
    "\n",
    "# *_pv_vw garantissent l'ordre donne par train/test lors de l'etape precedante \n",
    "# alors faire gaffe a ne pas le changer\n",
    "orb_train_x = np.multiply(orb_pv_vw.train_vw_freqs[...], orb_bovw.idf)\n",
    "orb_test_x = np.multiply(orb_pv_vw.test_vw_freqs[...], orb_bovw.idf)\n",
    "\n",
    "assert orb_train_x.shape[0] == train_y.shape[0]\n",
    "assert orb_test_x.shape[0] == test_y.shape[0]\n",
    "\n",
    "print(\"ORB PlantVillage train Visual Words\", orb_train_x.shape)\n",
    "print(\"ORB PlantVillage test Visual Words\", orb_train_x.shape)\n",
    "\n",
    "\n",
    "#\n",
    "# ORB KNN\n",
    "#\n",
    "orb_knn_pipe = load_knn(\"models/orb_knn.pkl\", orb_train_x, train_y)\n",
    "analysis_knn(\"ORB\", orb_knn_pipe, orb_test_x, test_y)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
